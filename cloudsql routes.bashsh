from google.cloud import monitoring_v3, bigquery, storage
from datetime import datetime, timedelta, timezone
import pandas as pd
import itertools, time, pickle

# =============================================================================
# BigQuery helpers
# =============================================================================

def ensure_bq_table(bq_project_id: str, dataset_id: str, table_name: str):
    client = bigquery.Client(project=bq_project_id)
    dataset_ref = f"{bq_project_id}.{dataset_id}"
    table_ref = f"{bq_project_id}.{dataset_id}.{table_name}"

    try:
        client.get_dataset(dataset_ref)
    except Exception:
        client.create_dataset(bigquery.Dataset(dataset_ref))

    try:
        client.get_table(table_ref)
    except Exception:
        schema = [
            bigquery.SchemaField("ds", "TIMESTAMP"),
            bigquery.SchemaField("y", "INT64"),
        ]
        table = bigquery.Table(table_ref, schema=schema)
        table.time_partitioning = bigquery.TimePartitioning(field="ds")
        client.create_table(table)

# =============================================================================
# Metric fetch
# =============================================================================

def fetch_pubsub_topic_send_count(
    metric_project_id: str,
    topic_id: str,
    metric_type: str,
    bq_project_id: str,
    dataset_id: str,
    table_name: str,
    lookback_hours: int
):
    metric_client = monitoring_v3.MetricServiceClient()

    now = datetime.now(timezone.utc)
    interval = monitoring_v3.TimeInterval(
        end_time=now,
        start_time=now - timedelta(hours=lookback_hours),
    )

    aggregation = monitoring_v3.Aggregation(
        alignment_period={"seconds": 60},  # ✅ FORCE 1 MIN
        per_series_aligner=monitoring_v3.Aggregation.Aligner.ALIGN_SUM,
    )

    metric_filter = (
        f'metric.type="{metric_type}" AND '
        f'resource.type="pubsub_topic" AND '
        f'resource.labels.topic_id="{topic_id}"'
    )

    results = metric_client.list_time_series(
        request={
            "name": f"projects/{metric_project_id}",
            "filter": metric_filter,
            "interval": interval,
            "aggregation": aggregation,
        }
    )

    rows = []
    for ts in results:
        for p in ts.points:
            raw = p.interval.end_time
            ds = datetime(
                raw.year, raw.month, raw.day,
                raw.hour, raw.minute, 0, tzinfo=timezone.utc
            )
            y = p.value.int64_value or int(p.value.double_value)
            rows.append({"ds": ds, "y": y})

    if not rows:
        print("No metric data.")
        return None

    df = pd.DataFrame(rows)
    df = df.groupby("ds", as_index=False)["y"].sum()  # safety

    ensure_bq_table(bq_project_id, dataset_id, table_name)

    bq = bigquery.Client(project=bq_project_id)
    bq.load_table_from_dataframe(
        df,
        f"{bq_project_id}.{dataset_id}.{table_name}",
        job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE"),
    ).result()

    return df

# =============================================================================
# SARIMAX training
# =============================================================================

def train_sarimax_from_bq_and_store(
    bq_project_id: str,
    dataset_id: str,
    table_name: str,
    max_models: int,
    max_seconds: int,
    storage_project_id: str
):
    from statsmodels.tsa.statespace.sarimax import SARIMAX

    MODEL_BUCKET = "trmc-12462064-observehub-dev-smart-analytics"

    bq = bigquery.Client(project=bq_project_id)
    df = bq.query(
        f"SELECT ds, y FROM `{bq_project_id}.{dataset_id}.{table_name}` ORDER BY ds"
    ).to_dataframe()

    df["ds"] = pd.to_datetime(df["ds"], utc=True)
    df = df.set_index("ds").asfreq("min")  # ✅ CRITICAL
    s = df["y"].astype(float)

    best_aic = float("inf")
    best_res = None

    start = time.time()
    attempts = 0

    for p, d, q in itertools.product([0,1,2], [0,1], [0,1,2]):
        if attempts >= max_models or time.time() - start > max_seconds:
            break
        try:
            res = SARIMAX(
                s,
                order=(p,d,q),
                enforce_stationarity=False,
                enforce_invertibility=False,
            ).fit(disp=False)
            attempts += 1

            if res.aic < best_aic:
                best_aic = res.aic
                best_res = res
        except Exception:
            attempts += 1

    if best_res is None:
        print("No model fit.")
        return

    local_path = f"{table_name}.pkl"
    best_res.save(local_path)  # ✅ NOT PICKLE

    storage.Client(storage_project_id)\
        .bucket(MODEL_BUCKET)\
        .blob(local_path)\
        .upload_from_filename(local_path)

    print(f"Model stored: AIC={best_aic:.2f}")