import requests
import csv

# Replace 'your_github_token' with your personal access token
GITHUB_TOKEN = 'your_github_token'
ORG_NAME = 'ihub-infrastructure'
SEARCH_TERM = 'efx'
CSV_FILE = 'efx_occurrences.csv'

headers = {'Authorization': f'token {GITHUB_TOKEN}'}

def list_repositories(org_name):
    """Lists all repositories in the specified GitHub organization."""
    repos = []
    url = f'https://api.github.com/orgs/{org_name}/repos'
    
    while url:
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            print(f"Error fetching repositories: {response.status_code} {response.text}")
            return repos
        
        repos.extend(response.json())
        # Check if there is a next page of results
        url = response.links.get('next', {}).get('url')
    
    return [repo['name'] for repo in repos]

def search_term_in_repo(org_name, repo_name, search_term):
    """Searches for a specific term in all files of a given repository."""
    url = f'https://api.github.com/search/code?q={search_term}+in:file+repo:{org_name}/{repo_name}'
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"Error searching in repo {repo_name}: {response.status_code} {response.text}")
        return []
    
    return response.json().get('items', [])

def get_last_committer(org_name, repo_name, file_path):
    """Gets the last committer for a specified file in a repository."""
    url = f'https://api.github.com/repos/{org_name}/{repo_name}/commits?path={file_path}'
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"Error fetching commits for file {file_path} in repo {repo_name}: {response.status_code} {response.text}")
        return None
    
    commits = response.json()
    if commits:
        return commits[0]['commit']['committer']['name']
    return None

def main():
    """Main function to gather data and write to a CSV file."""
    all_data = []
    repositories = list_repositories(ORG_NAME)
    
    if not repositories:
        print("No repositoriesâ¬¤



# Fetch the project name
PROJECT_ID=$(curl -H "Metadata-Flavor: Google" "http://metadata.google.internal/computeMetadata/v1/project/project-id")
PROJECT_NAME=$(gcloud projects describe $PROJECT_ID --format="value(name)")

# Extract the environment part (dev or prod)
ENVIRONMENT=$(echo $PROJECT_NAME | awk -F '-' '{print $NF}')

# Print the environment
echo "Environment: $ENVIRONMENT"

# If-else condition based on the environment
if [ "$ENVIRONMENT" == "dev" ]; then
    echo "This is the development environment."
    # Add your development environment-specific commands here
elif [ "$ENVIRONMENT" == "prod" ]; then
    echo "This is the production environment."
    # Add your production environment-specific commands here
else
    echo "Unknown environment: $ENVIRONMENT"
    # Add your handling for unknown environments here
fi




pipeline {
    agent any
    
    stages {
        stage('Check URL') {
            steps {
                script {
                    def responseCode = sh(script: "curl -o /dev/null -s -w '%{http_code}' https://yproject.uk.com", returnStdout: true).trim()
                    
                    if (responseCode == '200') {
                        echo 'yes'
                    } else {
                        echo 'no'
                    }
                }
            }
        }
    }
}



import requests
import csv

# Replace 'your_github_token' with your personal access token
GITHUB_TOKEN = 'your_github_token'
ORG_NAME = 'ihub-infrastructure'
SEARCH_TERM = 'efx'
CSV_FILE = 'efx_occurrences.csv'

headers = {'Authorization': f'token {GITHUB_TOKEN}'}

def list_repositories(org_name):
    repos = []
    url = f'https://api.github.com/orgs/{org_name}/repos'
    while url:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        repos.extend(response.json())
        url = response.links.get('next', {}).get('url')
    return [repo['name'] for repo in repos]

def search_term_in_repo(org_name, repo_name, search_term):
    url = f'https://api.github.com/search/code?q={search_term}+in:file+repo:{org_name}/{repo_name}'
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    return response.json().get('items', [])

def get_last_committer(org_name, repo_name, file_path):
    url = f'https://api.github.com/repos/{org_name}/{repo_name}/commits?path={file_path}'
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    commits = response.json()
    if commits:
        return commits[0]['commit']['committer']['name']
    return None

def main():
    all_data = []
    repositories = list_repositories(ORG_NAME)
    
    for repo in repositories:
        print(f"Searching in repository: {repo}")
        search_results = search_term_in_repo(ORG_NAME, repo, SEARCH_TERM)
        for result in search_results:
            file_path = result['path']
            last_committer = get_last_committer(ORG_NAME, repo, file_path)
            all_data.append([repo, file_path, last_committer])
    
    # Write data to CSV
    with open(CSV_FILE, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["Repository", "File Path", "Last Committer"])
        writer.writerows(all_data)
    
    print(f"Data has been written to {CSV_FILE}")

if __name__ == "__main__":
    main()
