Absolutely! Here’s a set of challenging Apache Spark interview questions with concise answers — covering core Spark, Spark SQL, DataFrames, RDDs, streaming, and optimization. These are evaluative and often asked in real interviews for Data Engineering roles.


---

We are in banking and we send transaction messages

So we get messages from source in json format in pubsub topic so we read messages through a dataflow rules pipeline and send it downstream to an api

So the way is we get a stm excel sheet which has transformation source columns and output columns what transformation we need to do like regex drop the field or change the type

Then we have a rules python code which onsisys of such functions like regex concat string to int or type transfer...

So we read stm and the functions and write a json config file like
{
Type - concat
Input column
Output col
}...
And pass this as an input params to the dataflow

Now I have already 20-25 such usecases , so now using ai I want to generate this json file automatically such that every time I pass something I get the json output as well as it train itself automatically so slowly it proves accurate outputs because obviously these stm and python functions are only for us and won't be same for other sin internet



[06/10, 02:01] Vaibhav Narkhede: What is shuffling 
Groupbykey and combine bykey
[06/10, 02:01] Vaibhav Narkhede: What are lambda functions and beam.map / beam.flatmap
[06/10, 02:02] Vaibhav Narkhede: What is yeild and how is differs from return
[06/10, 02:05] Vaibhav Narkhede: What happens if one of the element whole processing makes the process throw an exception
1️⃣ Spark Basics

Q1. Difference between RDD, DataFrame, and Dataset?
A:

RDD: Low-level, immutable distributed collection, strong typing, no optimizations.

DataFrame: Distributed collection of rows with schema, optimized via Catalyst.

Dataset: Strongly-typed, combines RDD type safety with DataFrame optimizations.


Q2. Explain lazy evaluation in Spark.
A: Spark builds a DAG of transformations but doesn’t execute until an action (e.g., count(), collect()) is called → optimizes execution and avoids unnecessary computations.

Q3. What is the difference between transformations and actions?
A:

Transformations: Lazy operations that return new RDD/DataFrame (map, filter, select).

Actions: Trigger execution and return results (count, collect, write).



---

2️⃣ Spark SQL / DataFrames

Q4. Difference between join and broadcast join?
A:

Regular join: Shuffles large tables → expensive.

Broadcast join: Small table broadcasted to all nodes → avoids shuffle → faster.


Q5. How does Spark SQL optimize queries?
A: Catalyst optimizer performs logical plan → optimized logical plan → physical plan → code generation, including predicate pushdown, column pruning, join reordering.

Q6. Explain partitioning and its impact on performance.
A:

Partitioning distributes data across nodes.

Proper partitioning reduces data shuffle, improves parallelism.

Skewed partitions cause slow tasks.



---

3️⃣ Spark Internals

Q7. What is a shuffle in Spark and why is it expensive?
A:

Shuffle = redistributing data across nodes (groupBy, join).

Expensive → disk I/O, network transfer, serialization/deserialization, and task waits.


Q8. Difference between narrow and wide transformations?
A:

Narrow: Each input partition → 1 output partition (map, filter) → no shuffle.

Wide: Input partition → multiple output partitions (groupBy, join) → requires shuffle.


Q9. Explain the DAG in Spark.
A: Directed Acyclic Graph of stages/tasks representing transformations.

DAG scheduler breaks jobs into stages, tasks execute in parallel.



---

4️⃣ Spark Streaming / Structured Streaming

Q10. Difference between batch and streaming in Spark?
A:

Batch → process all data at once.

Streaming → micro-batches (Structured Streaming) or continuous processing, low-latency.


Q11. How do you handle late-arriving data in streaming?
A:

Use event-time watermarking and window operations to include late events within allowed lateness.


Q12. Explain exactly-once semantics in Spark Structured Streaming.
A:

Achieved via idempotent writes + checkpointing → ensures each input is processed only once.



---

5️⃣ Performance & Optimization

Q13. How do you handle skewed data in joins or aggregations?
A:

Use salting, broadcast join, or repartition on join key to distribute data evenly.


Q14. Explain caching and persistence in Spark.
A:

cache() / persist() → store RDD/DataFrame in memory/disk to avoid recomputation.

Choose storage level (MEMORY_ONLY, MEMORY_AND_DISK) based on dataset size and reuse.


Q15. How does Spark manage memory for executors?
A:

JVM heap divided into:

Execution memory → shuffle, joins, aggregations

Storage memory → cached RDD/DataFrame


Spill to disk if memory exceeds.



---

6️⃣ Advanced / Scenario Questions

Q16. Difference between mapPartitions and map?
A:

mapPartitions: operates on entire partition → fewer function calls → more efficient.

map: operates row by row → higher function call overhead.


Q17. How to optimize Spark jobs on large datasets?
A:

Partition tuning (repartition, coalesce)

Avoid wide transformations / minimize shuffle

Use broadcast joins for small tables

Cache intermediate results

Enable whole-stage code generation


Q18. Explain lineage in Spark RDDs.
A:

Lineage = record of all transformations used to build an RDD.

Used for fault recovery: recompute lost partitions from parent RDDs.



---

If you want, I can create a second set of extremely hard Spark interview questions, including hands-on coding or scenario-based questions like “write a Spark job to deduplicate streaming events, partition by date, and store as Parquet with clustering,” which are often real evaluative tests in interviews.

Do you want me to make that next?


region = "us-west2"
instance_name = "psql-eu-west2-cl-dev"
metric_type = "cloudsql.googleapis.com/database/cpu/utilization"

metric_filter = (
    'resource.type = "cloudsql_database" '
    f'AND resource.label.database_id = "{project_id}:{region}:{instance_name}" '
    f'AND metric.type = "{metric_type}"'
)

aggregation = monitoring_v3.Aggregation(
    alignment_period={"seconds": 60},
    per_series_aligner=monitoring_v3.Aggregation.Aligner.ALIGN_MAX
)

interval = monitoring_v3.TimeInterval(
    end_time={"seconds": int(datetime.utcnow().timestamp())},
    start_time={"seconds": int((datetime.utcnow() - timedelta(hours=24)).timestamp())}
)

results = client.list_time_series(
    request={
        "name": f"projects/{project_id}",
        "filter": metric_filter,
        "interval": interval,
        "aggregation": aggregation,
        "view": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,
    }
)

.
